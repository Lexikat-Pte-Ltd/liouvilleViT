/media/cc/2T/liouvilleViT/train_masked_blocks.py:117: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()  # Initialize GradScaler for mixed precision

--- Epoch 1/100 ---
Batch shape: torch.Size([1, 10000, 2, 50, 50])
/media/cc/2T/liouvilleViT/train_masked_blocks.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 2: Loss = 0.285694
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 3: Loss = 0.284705
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 4: Loss = 0.283290
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 5: Loss = 0.282003
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 6: Loss = 0.280908
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 7: Loss = 0.279535
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 8: Loss = 0.278710
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 9: Loss = 0.277643
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 10: Loss = 0.276235
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 11: Loss = 0.275072
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 12: Loss = 0.274072
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
  Batch 13: Loss = 0.272761
Batch shape: torch.Size([1, 10000, 2, 50, 50])
MaskedBlockViT.forward called
  x_blocks shape: torch.Size([1, 10000, 2, 50, 50])
  CNNBlockEncoder input shape: torch.Size([10000, 2, 50, 50])
  CNNBlockEncoder output shape: torch.Size([10000, 128])
  x_emb shape after encoder and view: torch.Size([1, 10000, 128])
  x_emb shape after adding pos_embed_2d: torch.Size([1, 10000, 128])
  x_masked shape: torch.Size([1, 7000, 128])
  ids_keep shape: torch.Size([1, 7000])
  ids_mask shape: torch.Size([1, 3000])
  ids_restore shape: torch.Size([1, 10000])
  encoded shape after transformer: torch.Size([1, 7000, 128])
  decoder_input shape: torch.Size([1, 10000, 128])
  linear_decoded shape: torch.Size([1, 10000, 10816])
  conv_input shape: torch.Size([10000, 64, 13, 13])
  pred_flat_conv shape: torch.Size([10000, 2, 52, 52])
  Cropping pred_flat_conv from torch.Size([10000, 2, 52, 52]) to (2, 50, 50)
x_blocks.shape: torch.Size([1, 10000, 50, 50, 2])
Traceback (most recent call last):
  File "/media/cc/2T/liouvilleViT/train_masked_blocks.py", line 178, in <module>
    run_training()
  File "/media/cc/2T/liouvilleViT/train_masked_blocks.py", line 140, in run_training
    scaler.step(optimizer)
  File "/home/cc/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/cc/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/home/cc/.pyenv/versions/3.10.12/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
